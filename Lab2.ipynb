{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "#Load Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "raw_data = sc.textFile('/tmp/millionsong.txt')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "u'2001.0,0.884123733793,0.610454259079,0.600498416968,0.474669212493,0.247232680947,0.357306088914,0.344136412234,0.339641227335,0.600858840135,0.425704689024,0.60491501652,0.419193351817'"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "raw_data.first()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/panaseer/anaconda2/lib/python2.7/site-packages/matplotlib/font_manager.py:273: UserWarning: Matplotlib is building the font cache using fc-list. This may take a moment.\n",
      "  warnings.warn('Matplotlib is building the font cache using fc-list. This may take a moment.')\n"
     ]
    }
   ],
   "source": [
    "from pyspark.mllib.regression import LabeledPoint\n",
    "import numpy as np\n",
    "from matplotlib.colors import ListedColormap, Normalize\n",
    "from matplotlib.cm import get_cmap\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "df = raw_data.map(lambda x: x.split(\",\")).map(lambda x: LabeledPoint(x[0],x[1:])).toDF(['features','label'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+----------+----------+\n",
      "|MAX(label)|MIN(label)|\n",
      "+----------+----------+\n",
      "|    2011.0|    1922.0|\n",
      "+----------+----------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "df.selectExpr('MAX(label)','MIN(label)').show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Row(label=79.0, features=DenseVector([0.8841, 0.6105, 0.6005, 0.4747, 0.2472, 0.3573, 0.3441, 0.3396, 0.6009, 0.4257, 0.6049, 0.4192]))"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from pyspark.sql.functions import col\n",
    "\n",
    "parsed_df = df.select(col('label')-1922, 'features')\\\n",
    "              .withColumnRenamed(\"(label - 1922)\",'label')\n",
    "parsed_df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "DataFrame[label: double, features: vector]"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "train_df, val_df, test_df = parsed_df.randomSplit([0.8,0.1,0.1])\n",
    "\n",
    "train_df.cache() \n",
    "val_df.cache() \n",
    "test_df.cache()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "avg = test_df.agg({\"label\":\"mean\"}).map(lambda x: x[0]).collect()[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Baseline Test RMSE is equal to 21.5431516404\n"
     ]
    }
   ],
   "source": [
    "#baseline model - use average to predict\n",
    "\n",
    "from pyspark.ml.evaluation import RegressionEvaluator\n",
    "from pyspark.sql.functions import lit\n",
    "\n",
    "evaluator = RegressionEvaluator(predictionCol=\"prediction\")\n",
    "\n",
    "baseline_pred_label_df = train_df.select('label').withColumn('prediction',lit(avg))\n",
    "\n",
    "print \"Baseline Test RMSE is equal to %s\" %(evaluator.evaluate(baseline_pred_label_df)) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "#Gradient Descent by hand"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "from pyspark.mllib.linalg import DenseVector\n",
    "\n",
    "evaluator = RegressionEvaluator(predictionCol=\"prediction\")\n",
    "def calc_RMSE(dataset):\n",
    "    \"\"\"Calculates the root mean squared error for an dataset of (prediction, label) tuples.\n",
    "\n",
    "    Args:\n",
    "        dataset (DataFrame of (float, float)): A `DataFrame` consisting of (prediction, label) tuples.\n",
    "\n",
    "    Returns:\n",
    "        float: The square root of the mean of the squared errors.\n",
    "    \"\"\"\n",
    "#    evaluator = RegressionEvaluator(predictionCol=\"prediction\")\n",
    "    return evaluator.evaluate(dataset)\n",
    "\n",
    "def gradient_summand(weights, lp):\n",
    "    \"\"\"Calculates the gradient summand for a given weight and `LabeledPoint`.\"\"\"\n",
    "    summand = DenseVector((DenseVector.dot(lp.features,weights) - lp.label)*lp.features)\n",
    "    return summand\n",
    "\n",
    "def get_labeled_prediction(weights, observation):\n",
    "    \"\"\"Calculates predictions given a tuple of (labeledpoint,features) \n",
    "       and returns a (prediction, label) tuple.\"\"\"\n",
    "    \n",
    "    prediction = float(DenseVector.dot(DenseVector(weights),observation.features))\n",
    "    label = float(observation.label)\n",
    "    \n",
    "    return prediction,label"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(0.0, 79.0)"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "d = len(train_df.first().features)\n",
    "w = np.zeros(d)\n",
    "train_df.map(lambda x: get_labeled_prediction(w,x)).first()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "DenseVector([-71.8097, -49.933, -44.0369, -39.3628, -21.8353, -24.712, -35.4339, -35.4453, -51.3335, -38.6996, -46.7607, -35.5502])"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "train_df.map(lambda x: gradient_summand(w,x)).first()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(array([ 25.54267199,  24.28569575,  -3.73016841,   8.63990605,\n",
       "          6.56753476,  -8.49258512,  18.21909824,   2.12252844,\n",
       "          9.91043433,   4.86732809,  11.50202736,   1.9461365 ]),\n",
       " array([  58.07764142,  105.32248399,  111.01517104,   76.87970407,\n",
       "          39.39049974,   22.76882326,   20.40322172,   20.23965109,\n",
       "          20.16885342,   20.10417476,   20.04391336,   19.98743327,\n",
       "          19.9342358 ,   19.88392114,   19.83616298,   19.79069097,\n",
       "          19.74727821,   19.70573211,   19.66588755,   19.62760176,\n",
       "          19.59075028,   19.55522389,   19.52092612,   19.48777128,\n",
       "          19.45568284,   19.42459216,   19.39443735,   19.36516245,\n",
       "          19.33671663,   19.30905356,   19.28213089,   19.25590979,\n",
       "          19.23035456,   19.20543228,   19.18111252,   19.15736711,\n",
       "          19.13416988,   19.11149651,   19.08932431,   19.06763209,\n",
       "          19.04640006,   19.02560965,   19.00524345,   18.98528508,\n",
       "          18.96571915,   18.94653113,   18.92770733,   18.90923481,\n",
       "          18.89110132,   18.87329527,   18.85580567,   18.8386221 ,\n",
       "          18.82173463,   18.80513386,   18.7888108 ,   18.77275693,\n",
       "          18.7569641 ,   18.74142453,   18.7261308 ,   18.71107582,\n",
       "          18.69625281,   18.68165528,   18.66727699,   18.65311201,\n",
       "          18.63915459,   18.62539927,   18.61184077,   18.59847403,\n",
       "          18.58529419,   18.57229656,   18.55947664,   18.54683009,\n",
       "          18.53435274,   18.52204054,   18.50988963,   18.49789624,\n",
       "          18.48605677,   18.47436771,   18.4628257 ,   18.45142746,\n",
       "          18.44016984,   18.4290498 ,   18.41806438,   18.40721071,\n",
       "          18.39648604,   18.38588767,   18.37541301,   18.36505954,\n",
       "          18.35482481,   18.34470646,   18.33470218,   18.32480973,\n",
       "          18.31502696,   18.30535175,   18.29578205,   18.28631588,\n",
       "          18.27695131,   18.26768644,   18.25851946,   18.24944858]))"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "def linreg_gradient_descent(train_data, num_iters):\n",
    "    \"\"\"Calculates the weights and error for a linear regression model trained with gradient descent.\n",
    "\n",
    "    Returns a tuple of (weights, training errors).  Weights will be the\n",
    "            final weights (one weight per feature) for the model, and training errors will contain\n",
    "            an error (RMSE) for each iteration of the algorithm.\n",
    "    \"\"\"\n",
    "    # The length of the training data\n",
    "    n = train_data.count()\n",
    "    # The number of features in the training data\n",
    "    d = len(train_data.first().features)\n",
    "    w = np.zeros(d)\n",
    "    alpha = 1.0\n",
    "    # We will compute and store the training error after each iteration\n",
    "    error_train = np.zeros(num_iters)\n",
    "    for i in range(num_iters):\n",
    "        preds_and_labels_train = train_data.map(lambda x: get_labeled_prediction(w,x))\n",
    "        preds_and_labels_train_df = preds_and_labels_train.toDF([\"prediction\", \"label\"])\n",
    "        error_train[i] = calc_RMSE(preds_and_labels_train_df)\n",
    "\n",
    "        # Calculate the `gradient`.  Make use of the `gradient_summand` function you wrote in (3a).\n",
    "        # Note that `gradient` should be a `DenseVector` of length `d`.\n",
    "        gradient = train_data.map(lambda x: gradient_summand(w,x)).sum()\n",
    "\n",
    "        # Update the weights\n",
    "        alpha_i = alpha / (n * np.sqrt(i+1))\n",
    "        w = w - alpha_i * gradient\n",
    "        \n",
    "    return w, error_train\n",
    "    \n",
    "\n",
    "linreg_gradient_descent(train_df, 100)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "####  Train the model\n",
    "#### Now let's train a linear regression model on all of our training data and evaluate its accuracy on the validation set.\n",
    "#### Note that the test set will not be used here. If we evaluated the model on the test set, we would bias our final results."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Validation RMSE:\n",
      "\tBaseline = 53.403\n",
      "\tLR0 = 18.403\n"
     ]
    }
   ],
   "source": [
    "num_iters = 50\n",
    "weights_LR0, error_train_LR0 = linreg_gradient_descent(train_df,num_iters)\n",
    "\n",
    "preds_and_labels = (val_df\n",
    "                      .map(lambda x: get_labeled_prediction(weights_LR0,x)))\n",
    "preds_and_labels_df = sqlContext.createDataFrame(preds_and_labels, [\"prediction\", \"label\"])\n",
    "rmse_val_LR0 = calc_RMSE(preds_and_labels_df)\n",
    "\n",
    "print 'Validation RMSE:\\n\\tBaseline = {0:.3f}\\n\\tLR0 = {1:.3f}'.format(avg,\n",
    "                                                                       rmse_val_LR0)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# MLlib implemenatation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[22.7826076628,27.2671667664,-66.4716850827,53.9043829814,-8.77708333401,-49.3084390785,32.6676055812,-22.0419154042,3.06020516062,-4.62732222397,-11.8928196002,-13.7315891729] 65.6379221505\n"
     ]
    }
   ],
   "source": [
    "from pyspark.ml.regression import LinearRegression\n",
    "# Values to use when training the linear regression model\n",
    "\n",
    "num_iters = 500  # iterations\n",
    "reg = 1e-1  # regParam\n",
    "alpha = .2  # elasticNetParam\n",
    "use_intercept = True  # intercept\n",
    "\n",
    "# TODO: Replace <FILL IN> with appropriate code\n",
    "lin_reg = LinearRegression(maxIter=num_iters, regParam=reg, \n",
    "                           elasticNetParam=0.1, fitIntercept=True)\n",
    "first_model = lin_reg.fit(train_df)\n",
    "\n",
    "# coeffsLR1 stores the model coefficients; interceptLR1 stores the model intercept\n",
    "coeffs_LR1 = first_model.coefficients\n",
    "intercept_LR1 = first_model.intercept\n",
    "print coeffs_LR1, intercept_LR1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/usr/hdp/2.4.0.0-169/spark/python/pyspark/ml/regression.py:123: UserWarning: weights is deprecated. Use coefficients instead.\n",
      "  warnings.warn(\"weights is deprecated. Use coefficients instead.\")\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>label</th>\n",
       "      <th>prediction</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>79</td>\n",
       "      <td>66.802510</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>79</td>\n",
       "      <td>69.722341</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>79</td>\n",
       "      <td>64.049553</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>79</td>\n",
       "      <td>67.092058</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>79</td>\n",
       "      <td>64.942050</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>79</td>\n",
       "      <td>66.935711</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6</th>\n",
       "      <td>85</td>\n",
       "      <td>70.471380</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7</th>\n",
       "      <td>80</td>\n",
       "      <td>60.532097</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8</th>\n",
       "      <td>82</td>\n",
       "      <td>49.532286</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9</th>\n",
       "      <td>81</td>\n",
       "      <td>64.972118</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   label  prediction\n",
       "0     79   66.802510\n",
       "1     79   69.722341\n",
       "2     79   64.049553\n",
       "3     79   67.092058\n",
       "4     79   64.942050\n",
       "5     79   66.935711\n",
       "6     85   70.471380\n",
       "7     80   60.532097\n",
       "8     82   49.532286\n",
       "9     81   64.972118"
      ]
     },
     "execution_count": 17,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "pred = first_model.transform(train_df)\n",
    "pred.select('label','prediction').toPandas().head(10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {
    "collapsed": false,
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "from pyspark.sql import Row\n",
    "validation = first_model.transform(val_df).select('prediction','label')\n",
    "rmse_val_LR1 = evaluator.evaluate(validation)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "53.4030075188 18.4027353417 15.5050063501\n"
     ]
    }
   ],
   "source": [
    "print avg, rmse_val_LR0, rmse_val_LR1"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Grid Search"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Validation RMSE:\n",
      "\t\tBaseline = 53.403\n",
      "\t\tLR0 = 18.403\n",
      "\t\tLR1 = 15.505\n",
      "\t\tLRGrid = 15.505\n"
     ]
    }
   ],
   "source": [
    "best_RMSE = rmse_val_LR1\n",
    "best_reg_param = reg\n",
    "best_model = first_model\n",
    "\n",
    "num_iters = 500  # iterations\n",
    "alpha = .2  # elasticNetParam\n",
    "use_intercept = True  # intercept\n",
    "\n",
    "for reg in [1e-10,1e-5,.99]:\n",
    "    lin_reg = LinearRegression(maxIter=num_iters, regParam=reg, elasticNetParam=alpha, fitIntercept=use_intercept)\n",
    "    model = lin_reg.fit(train_df)\n",
    "    val_pred_df = model.transform(val_df)\n",
    "\n",
    "    rmse_val_grid = evaluator.evaluate(val_pred_df)\n",
    "\n",
    "    if rmse_val_grid < best_RMSE:\n",
    "        best_RMSE = rmse_val_grid\n",
    "        best_reg_param = reg\n",
    "        best_model = model\n",
    "\n",
    "rmse_val_LR_grid = best_RMSE\n",
    "\n",
    "print ('Validation RMSE:\\n\\t\\tBaseline = {0:.3f}\\n\\t\\tLR0 = {1:.3f}\\n\\t\\tLR1 = {2:.3f}\\n' +\n",
    "       '\\t\\tLRGrid = {3:.3f}').format(avg, rmse_val_LR0, rmse_val_LR1, rmse_val_LR_grid)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 76,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "from itertools import product\n",
    "\n",
    "def two_way_interactions(lp):\n",
    "    \"\"\"Creates a new `LabeledPoint` that includes two-way interactions.\n",
    "\n",
    "    Note:\n",
    "        For features [x, y] the two-way interactions would be [x^2, x*y, y*x, y^2] and these\n",
    "        would be appended to the original [x, y] feature list.\n",
    "\n",
    "    Args:\n",
    "        lp (LabeledPoint): The label and features for this observation.\n",
    "\n",
    "    Returns:\n",
    "        LabeledPoint: The new `LabeledPoint` should have the same label as `lp`.  Its features\n",
    "            should include the features from `lp` followed by the two-way interaction features.\n",
    "    \"\"\"\n",
    "    i_j = list(product(range(len(lp.features)),range(len(lp.features))))\n",
    "    \n",
    "    two_way = LabeledPoint(lp.label, np.hstack((lp.features,[lp.features[i] * lp.features[j] for (i,j) in i_j])))\n",
    "    \n",
    "    return two_way"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 86,
   "metadata": {
    "collapsed": false,
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "two_way_train_df = train_df.map(lambda x: LabeledPoint(x[0],x[1]))\\\n",
    "                           .map(lambda x: two_way_interactions(x)).toDF(['features','label'])\n",
    "two_way_val_df = val_df.map(lambda x: LabeledPoint(x[0],x[1]))\\\n",
    "                           .map(lambda x: two_way_interactions(x)).toDF(['features','label'])\n",
    "two_way_test_df = test_df.map(lambda x: LabeledPoint(x[0],x[1]))\\\n",
    "                           .map(lambda x: two_way_interactions(x)).toDF(['features','label'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 87,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Row(features=DenseVector([0.909, 0.6321, 0.5574, 0.4983, 0.2764, 0.3128, 0.4485, 0.4487, 0.6498, 0.4899, 0.5919, 0.45, 0.8263, 0.5745, 0.5067, 0.4529, 0.2512, 0.2843, 0.4077, 0.4078, 0.5906, 0.4453, 0.538, 0.409, 0.5745, 0.3995, 0.3523, 0.3149, 0.1747, 0.1977, 0.2835, 0.2836, 0.4107, 0.3096, 0.3741, 0.2844, 0.5067, 0.3523, 0.3107, 0.2777, 0.1541, 0.1744, 0.25, 0.2501, 0.3622, 0.2731, 0.3299, 0.2508, 0.4529, 0.3149, 0.2777, 0.2483, 0.1377, 0.1559, 0.2235, 0.2236, 0.3238, 0.2441, 0.2949, 0.2242, 0.2512, 0.1747, 0.1541, 0.1377, 0.0764, 0.0865, 0.124, 0.124, 0.1796, 0.1354, 0.1636, 0.1244, 0.2843, 0.1977, 0.1744, 0.1559, 0.0865, 0.0979, 0.1403, 0.1403, 0.2033, 0.1532, 0.1852, 0.1408, 0.4077, 0.2835, 0.25, 0.2235, 0.124, 0.1403, 0.2012, 0.2012, 0.2915, 0.2197, 0.2655, 0.2018, 0.4078, 0.2836, 0.2501, 0.2236, 0.124, 0.1403, 0.2012, 0.2013, 0.2915, 0.2198, 0.2656, 0.2019, 0.5906, 0.4107, 0.3622, 0.3238, 0.1796, 0.2033, 0.2915, 0.2915, 0.4222, 0.3183, 0.3846, 0.2924, 0.4453, 0.3096, 0.2731, 0.2441, 0.1354, 0.1532, 0.2197, 0.2198, 0.3183, 0.24, 0.29, 0.2204, 0.538, 0.3741, 0.3299, 0.2949, 0.1636, 0.1852, 0.2655, 0.2656, 0.3846, 0.29, 0.3504, 0.2664, 0.409, 0.2844, 0.2508, 0.2242, 0.1244, 0.1408, 0.2018, 0.2019, 0.2924, 0.2204, 0.2664, 0.2025]), label=79.0)"
      ]
     },
     "execution_count": 87,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "two_way_train_df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 89,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Validation RMSE:\n",
      "\tBaseline = 53.403\n",
      "\tLR0 = 18.403\n",
      "\tLR1 = 15.505\n",
      "\tLRGrid = 15.505\n",
      "\tLRInteract = 14.882\n"
     ]
    }
   ],
   "source": [
    "num_iters = 500\n",
    "reg = 1e-10\n",
    "alpha = .2\n",
    "use_intercept = True\n",
    "\n",
    "lin_reg = LinearRegression(maxIter=num_iters, regParam=reg, elasticNetParam=alpha, fitIntercept=use_intercept)\n",
    "model_interact = lin_reg.fit(two_way_train_df)\n",
    "preds_and_labels_interact_df = model_interact.transform(two_way_val_df)\n",
    "rmse_val_interact = evaluator.evaluate(preds_and_labels_interact_df)\n",
    "\n",
    "print ('Validation RMSE:\\n\\tBaseline = {0:.3f}\\n\\tLR0 = {1:.3f}\\n\\tLR1 = {2:.3f}\\n\\tLRGrid = ' +\n",
    "       '{3:.3f}\\n\\tLRInteract = {4:.3f}').format(avg, rmse_val_LR0, rmse_val_LR1,\n",
    "                                                 rmse_val_LR_grid, rmse_val_interact)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 92,
   "metadata": {
    "collapsed": false,
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Test RMSE:\n",
      "\tBaseline = 53.403\n",
      "\tLRInteract = 14.487\n"
     ]
    }
   ],
   "source": [
    "preds_and_labels_test_df = model_interact.transform(two_way_test_df)\n",
    "rmse_test_interact = evaluator.evaluate(preds_and_labels_test_df)\n",
    "\n",
    "print ('Test RMSE:\\n\\tBaseline = {0:.3f}\\n\\tLRInteract = {1:.3f}'\n",
    "       .format(avg, rmse_test_interact))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 94,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "ename": "Py4JJavaError",
     "evalue": "An error occurred while calling o8828.transform.\n: java.util.NoSuchElementException: Failed to find a default value for inputCol\n\tat org.apache.spark.ml.param.Params$$anonfun$getOrDefault$2.apply(params.scala:647)\n\tat org.apache.spark.ml.param.Params$$anonfun$getOrDefault$2.apply(params.scala:647)\n\tat scala.Option.getOrElse(Option.scala:120)\n\tat org.apache.spark.ml.param.Params$class.getOrDefault(params.scala:646)\n\tat org.apache.spark.ml.PipelineStage.getOrDefault(Pipeline.scala:43)\n\tat org.apache.spark.ml.param.Params$class.$(params.scala:651)\n\tat org.apache.spark.ml.PipelineStage.$(Pipeline.scala:43)\n\tat org.apache.spark.ml.UnaryTransformer.transformSchema(Transformer.scala:106)\n\tat org.apache.spark.ml.PipelineStage.transformSchema(Pipeline.scala:68)\n\tat org.apache.spark.ml.UnaryTransformer.transform(Transformer.scala:117)\n\tat sun.reflect.NativeMethodAccessorImpl.invoke0(Native Method)\n\tat sun.reflect.NativeMethodAccessorImpl.invoke(NativeMethodAccessorImpl.java:62)\n\tat sun.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:43)\n\tat java.lang.reflect.Method.invoke(Method.java:498)\n\tat py4j.reflection.MethodInvoker.invoke(MethodInvoker.java:231)\n\tat py4j.reflection.ReflectionEngine.invoke(ReflectionEngine.java:381)\n\tat py4j.Gateway.invoke(Gateway.java:259)\n\tat py4j.commands.AbstractCommand.invokeMethod(AbstractCommand.java:133)\n\tat py4j.commands.CallCommand.execute(CallCommand.java:79)\n\tat py4j.GatewayConnection.run(GatewayConnection.java:209)\n\tat java.lang.Thread.run(Thread.java:745)\n",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mPy4JJavaError\u001b[0m                             Traceback (most recent call last)",
      "\u001b[1;32m<ipython-input-94-cc14e54c5de6>\u001b[0m in \u001b[0;36m<module>\u001b[1;34m()\u001b[0m\n\u001b[0;32m     12\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     13\u001b[0m \u001b[0mpipeline\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mPipeline\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mstages\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;33m[\u001b[0m\u001b[0mpolynomial_expansion\u001b[0m\u001b[1;33m,\u001b[0m\u001b[0mlinear_regression\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m---> 14\u001b[1;33m \u001b[0mpipeline_model\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mpipeline\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mfit\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mtrain_df\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m     15\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     16\u001b[0m \u001b[0mpredictions_df\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mpipeline_model\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mtransform\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mtest_df\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m/usr/hdp/2.4.0.0-169/spark/python/pyspark/ml/pipeline.py\u001b[0m in \u001b[0;36mfit\u001b[1;34m(self, dataset, params)\u001b[0m\n\u001b[0;32m     67\u001b[0m                 \u001b[1;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mcopy\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mparams\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_fit\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mdataset\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     68\u001b[0m             \u001b[1;32melse\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m---> 69\u001b[1;33m                 \u001b[1;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_fit\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mdataset\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m     70\u001b[0m         \u001b[1;32melse\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     71\u001b[0m             raise ValueError(\"Params must be either a param map or a list/tuple of param maps, \"\n",
      "\u001b[1;32m/usr/hdp/2.4.0.0-169/spark/python/pyspark/ml/pipeline.py\u001b[0m in \u001b[0;36m_fit\u001b[1;34m(self, dataset)\u001b[0m\n\u001b[0;32m    209\u001b[0m                 \u001b[1;32mif\u001b[0m \u001b[0misinstance\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mstage\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mTransformer\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    210\u001b[0m                     \u001b[0mtransformers\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mappend\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mstage\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 211\u001b[1;33m                     \u001b[0mdataset\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mstage\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mtransform\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mdataset\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    212\u001b[0m                 \u001b[1;32melse\u001b[0m\u001b[1;33m:\u001b[0m  \u001b[1;31m# must be an Estimator\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    213\u001b[0m                     \u001b[0mmodel\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mstage\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mfit\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mdataset\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m/usr/hdp/2.4.0.0-169/spark/python/pyspark/ml/pipeline.py\u001b[0m in \u001b[0;36mtransform\u001b[1;34m(self, dataset, params)\u001b[0m\n\u001b[0;32m    112\u001b[0m                 \u001b[1;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mcopy\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mparams\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_transform\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mdataset\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    113\u001b[0m             \u001b[1;32melse\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 114\u001b[1;33m                 \u001b[1;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_transform\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mdataset\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    115\u001b[0m         \u001b[1;32melse\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    116\u001b[0m             \u001b[1;32mraise\u001b[0m \u001b[0mValueError\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;34m\"Params must be either a param map but got %s.\"\u001b[0m \u001b[1;33m%\u001b[0m \u001b[0mtype\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mparams\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m/usr/hdp/2.4.0.0-169/spark/python/pyspark/ml/wrapper.py\u001b[0m in \u001b[0;36m_transform\u001b[1;34m(self, dataset)\u001b[0m\n\u001b[0;32m    146\u001b[0m     \u001b[1;32mdef\u001b[0m \u001b[0m_transform\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mdataset\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    147\u001b[0m         \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_transfer_params_to_java\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 148\u001b[1;33m         \u001b[1;32mreturn\u001b[0m \u001b[0mDataFrame\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_java_obj\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mtransform\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mdataset\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_jdf\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mdataset\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0msql_ctx\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    149\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    150\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m/usr/hdp/2.4.0.0-169/spark/python/lib/py4j-0.9-src.zip/py4j/java_gateway.py\u001b[0m in \u001b[0;36m__call__\u001b[1;34m(self, *args)\u001b[0m\n\u001b[0;32m    811\u001b[0m         \u001b[0manswer\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mgateway_client\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0msend_command\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mcommand\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    812\u001b[0m         return_value = get_return_value(\n\u001b[1;32m--> 813\u001b[1;33m             answer, self.gateway_client, self.target_id, self.name)\n\u001b[0m\u001b[0;32m    814\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    815\u001b[0m         \u001b[1;32mfor\u001b[0m \u001b[0mtemp_arg\u001b[0m \u001b[1;32min\u001b[0m \u001b[0mtemp_args\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m/usr/hdp/2.4.0.0-169/spark/python/pyspark/sql/utils.py\u001b[0m in \u001b[0;36mdeco\u001b[1;34m(*a, **kw)\u001b[0m\n\u001b[0;32m     43\u001b[0m     \u001b[1;32mdef\u001b[0m \u001b[0mdeco\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m*\u001b[0m\u001b[0ma\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;33m**\u001b[0m\u001b[0mkw\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     44\u001b[0m         \u001b[1;32mtry\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m---> 45\u001b[1;33m             \u001b[1;32mreturn\u001b[0m \u001b[0mf\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m*\u001b[0m\u001b[0ma\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;33m**\u001b[0m\u001b[0mkw\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m     46\u001b[0m         \u001b[1;32mexcept\u001b[0m \u001b[0mpy4j\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mprotocol\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mPy4JJavaError\u001b[0m \u001b[1;32mas\u001b[0m \u001b[0me\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     47\u001b[0m             \u001b[0ms\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0me\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mjava_exception\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mtoString\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m/usr/hdp/2.4.0.0-169/spark/python/lib/py4j-0.9-src.zip/py4j/protocol.py\u001b[0m in \u001b[0;36mget_return_value\u001b[1;34m(answer, gateway_client, target_id, name)\u001b[0m\n\u001b[0;32m    306\u001b[0m                 raise Py4JJavaError(\n\u001b[0;32m    307\u001b[0m                     \u001b[1;34m\"An error occurred while calling {0}{1}{2}.\\n\"\u001b[0m\u001b[1;33m.\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 308\u001b[1;33m                     format(target_id, \".\", name), value)\n\u001b[0m\u001b[0;32m    309\u001b[0m             \u001b[1;32melse\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    310\u001b[0m                 raise Py4JError(\n",
      "\u001b[1;31mPy4JJavaError\u001b[0m: An error occurred while calling o8828.transform.\n: java.util.NoSuchElementException: Failed to find a default value for inputCol\n\tat org.apache.spark.ml.param.Params$$anonfun$getOrDefault$2.apply(params.scala:647)\n\tat org.apache.spark.ml.param.Params$$anonfun$getOrDefault$2.apply(params.scala:647)\n\tat scala.Option.getOrElse(Option.scala:120)\n\tat org.apache.spark.ml.param.Params$class.getOrDefault(params.scala:646)\n\tat org.apache.spark.ml.PipelineStage.getOrDefault(Pipeline.scala:43)\n\tat org.apache.spark.ml.param.Params$class.$(params.scala:651)\n\tat org.apache.spark.ml.PipelineStage.$(Pipeline.scala:43)\n\tat org.apache.spark.ml.UnaryTransformer.transformSchema(Transformer.scala:106)\n\tat org.apache.spark.ml.PipelineStage.transformSchema(Pipeline.scala:68)\n\tat org.apache.spark.ml.UnaryTransformer.transform(Transformer.scala:117)\n\tat sun.reflect.NativeMethodAccessorImpl.invoke0(Native Method)\n\tat sun.reflect.NativeMethodAccessorImpl.invoke(NativeMethodAccessorImpl.java:62)\n\tat sun.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:43)\n\tat java.lang.reflect.Method.invoke(Method.java:498)\n\tat py4j.reflection.MethodInvoker.invoke(MethodInvoker.java:231)\n\tat py4j.reflection.ReflectionEngine.invoke(ReflectionEngine.java:381)\n\tat py4j.Gateway.invoke(Gateway.java:259)\n\tat py4j.commands.AbstractCommand.invokeMethod(AbstractCommand.java:133)\n\tat py4j.commands.CallCommand.execute(CallCommand.java:79)\n\tat py4j.GatewayConnection.run(GatewayConnection.java:209)\n\tat java.lang.Thread.run(Thread.java:745)\n"
     ]
    }
   ],
   "source": [
    "from pyspark.ml import Pipeline\n",
    "from pyspark.ml.feature import PolynomialExpansion\n",
    "\n",
    "num_iters = 500\n",
    "reg = 1e-10\n",
    "alpha = .2\n",
    "use_intercept = True\n",
    "\n",
    "polynomial_expansion = PolynomialExpansion(degree=2)\n",
    "linear_regression = LinearRegression(maxIter=num_iters, regParam=reg, elasticNetParam=alpha,\n",
    "                                     fitIntercept=use_intercept, featuresCol='polyFeatures')\n",
    "\n",
    "pipeline = Pipeline(stages=[polynomial_expansion,linear_regression])\n",
    "pipeline_model = pipeline.fit(train_df)\n",
    "\n",
    "predictions_df = pipeline_model.transform(test_df)\n",
    "\n",
    "evaluator = RegressionEvaluator()\n",
    "rmse_test_pipeline = evaluator.evaluate(predictions_df, {evaluator.metricName: \"rmse\"})\n",
    "print('RMSE for test data set using pipelines: {0:.3f}'.format(rmse_test_pipeline))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 2",
   "language": "python",
   "name": "python2"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.11"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
