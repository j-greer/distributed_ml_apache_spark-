{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "#Load Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "raw_data = sc.textFile('millionsong.txt')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "u'2001.0,0.884123733793,0.610454259079,0.600498416968,0.474669212493,0.247232680947,0.357306088914,0.344136412234,0.339641227335,0.600858840135,0.425704689024,0.60491501652,0.419193351817'"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "raw_data.first()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "from pyspark.mllib.regression import LabeledPoint\n",
    "import numpy as np\n",
    "\n",
    "df = raw_data.map(lambda x: x.split(\",\")).map(lambda x: LabeledPoint(x[0],x[1:])).toDF(['features','label'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-----------+-----------+\n",
      "|'MAX(label)|'MIN(label)|\n",
      "+-----------+-----------+\n",
      "|     2011.0|     1922.0|\n",
      "+-----------+-----------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "df.selectExpr('MAX(label)','MIN(label)').show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Row(label=79.0, features=DenseVector([0.8841, 0.6105, 0.6005, 0.4747, 0.2472, 0.3573, 0.3441, 0.3396, 0.6009, 0.4257, 0.6049, 0.4192]))"
      ]
     },
     "execution_count": 23,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from pyspark.sql.functions import col\n",
    "\n",
    "parsed_df = df.select(col('label')-1922, 'features')\\\n",
    "              .withColumnRenamed(\"(label - 1922)\",'label')\n",
    "parsed_df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "DataFrame[label: double, features: vector]"
      ]
     },
     "execution_count": 25,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "train_df, val_df, test_df = parsed_df.randomSplit([0.8,0.1,0.1])\n",
    "\n",
    "train_df.cache() \n",
    "val_df.cache() \n",
    "test_df.cache()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 59,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "avg = test_df.agg({\"label\":\"mean\"}).map(lambda x: x[0]).collect()[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 70,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Baseline Test RMSE is equal to 21.4324898207\n"
     ]
    }
   ],
   "source": [
    "#baseline model - use average to predict\n",
    "\n",
    "from pyspark.ml.evaluation import RegressionEvaluator\n",
    "from pyspark.sql.functions import lit\n",
    "\n",
    "evaluator = RegressionEvaluator(predictionCol=\"prediction\")\n",
    "\n",
    "baseline_pred_label_df = train_df.select('label').withColumn('prediction',lit(avg))\n",
    "\n",
    "print \"Baseline Test RMSE is equal to %s\" %(evaluator.evaluate(baseline_pred_label_df)) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 71,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "#Gradient Descent by hand"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 73,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "from pyspark.mllib.linalg import DenseVector\n",
    "\n",
    "def gradient_summand(weights, lp):\n",
    "    \"\"\"Calculates the gradient summand for a given weight and `LabeledPoint`.\"\"\"\n",
    "    summand = DenseVector((DenseVector.dot(lp.features,weights) - lp.label)*lp.features)\n",
    "    return summand\n",
    "\n",
    "def get_labeled_prediction(weights, observation):\n",
    "    \"\"\"Calculates predictions given a tuple of (labeledpoint,features) \n",
    "       and returns a (prediction, label) tuple.\"\"\"\n",
    "    \n",
    "    prediction = float(DenseVector.dot(DenseVector(weights),observation.features))\n",
    "    label = float(observation.label)\n",
    "    \n",
    "    return prediction,label"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 77,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(0.0, 79.0)"
      ]
     },
     "execution_count": 77,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "d = len(train_df.first().features)\n",
    "w = np.zeros(d)\n",
    "train_df.map(lambda x: get_labeled_prediction(w,x)).first()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 78,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "DenseVector([-69.8458, -48.2259, -47.4394, -37.4989, -19.5314, -28.2272, -27.1868, -26.8317, -47.4678, -33.6307, -47.7883, -33.1163])"
      ]
     },
     "execution_count": 78,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "train_df.map(lambda x: gradient_summand(w,x)).first()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "def linreg_gradient_descent(train_data, num_iters):\n",
    "    \"\"\"Calculates the weights and error for a linear regression model trained with gradient descent.\n",
    "\n",
    "    Note:\n",
    "        `DenseVector` behaves similarly to a `numpy.ndarray` and they can be used interchangably\n",
    "        within this function.  For example, they both implement the `dot` method.\n",
    "\n",
    "    Args:\n",
    "        train_data (RDD of LabeledPoint): The labeled data for use in training the model.\n",
    "        num_iters (int): The number of iterations of gradient descent to perform.\n",
    "\n",
    "    Returns:\n",
    "        (np.ndarray, np.ndarray): A tuple of (weights, training errors).  Weights will be the\n",
    "            final weights (one weight per feature) for the model, and training errors will contain\n",
    "            an error (RMSE) for each iteration of the algorithm.\n",
    "    \"\"\"\n",
    "    # The length of the training data\n",
    "    n = train_data.count()\n",
    "    # The number of features in the training data\n",
    "    d = len(train_data.first().features)\n",
    "    w = np.zeros(d)\n",
    "    alpha = 1.0\n",
    "    # We will compute and store the training error after each iteration\n",
    "    error_train = np.zeros(num_iters)\n",
    "    for i in range(num_iters):\n",
    "        # Use get_labeled_prediction from (3b) with trainData to obtain an RDD of (label, prediction)\n",
    "        # tuples.  Note that the weights all equal 0 for the first iteration, so the predictions will\n",
    "        # have large errors to start.\n",
    "        preds_and_labels_train = train_data.map(lambda x: get_labeled_prediction(w,x))\n",
    "        preds_and_labels_train_df = preds_and_labels_train.toDF([\"prediction\", \"label\"])\n",
    "        #error_train[i] = calc_RMSE(preds_and_labels_train_df)\n",
    "\n",
    "        # Calculate the `gradient`.  Make use of the `gradient_summand` function you wrote in (3a).\n",
    "        # Note that `gradient` should be a `DenseVector` of length `d`.\n",
    "        gradient = train_data.map(lambda x: gradient_summand(w,x)).collect()\n",
    "\n",
    "        # Update the weights\n",
    "        alpha_i = alpha / (n * np.sqrt(i+1))\n",
    "        w = w - alpha_i #* gradient\n",
    "        \n",
    "    #return w, error_train\n",
    "    return len(gradient)\n",
    "\n",
    "linreg_gradient_descent(train_df, 1000)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 2",
   "language": "python",
   "name": "python2"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.11"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
