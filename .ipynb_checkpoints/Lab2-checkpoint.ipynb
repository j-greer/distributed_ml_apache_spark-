{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Linear Regression\n",
    "\n",
    "Using a subset of the Million Song Dataset from the UCI Machine Learning Repository. Our goal is to train a linear regression model to predict the release year of a song given a set of audio features."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 73,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "from pyspark.mllib.regression import LabeledPoint\n",
    "from pyspark.ml import Pipeline\n",
    "from pyspark.ml.feature import PolynomialExpansion\n",
    "from pyspark.ml.regression import LinearRegression\n",
    "from pyspark.ml.evaluation import RegressionEvaluator\n",
    "from pyspark.sql.functions import col, lit\n",
    "from pyspark.sql import Row\n",
    "import numpy as np"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 56,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "u'2001.0,0.884123733793,0.610454259079,0.600498416968,0.474669212493,0.247232680947,0.357306088914,0.344136412234,0.339641227335,0.600858840135,0.425704689024,0.60491501652,0.419193351817'"
      ]
     },
     "execution_count": 56,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#Load Data\n",
    "raw_data = sc.textFile('millionsong.txt')\n",
    "raw_data.first()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 57,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "#Convert RDD into DataFrame with of two columns. One for the label (year the song was made)\n",
    "#and one for the features (a vector of the regressor variables)\n",
    "\n",
    "df = raw_data.map(lambda x: x.split(\",\")).map(lambda x: LabeledPoint(x[0],x[1:])).toDF(['features','label'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 58,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+----------+----------+\n",
      "|MAX(label)|MIN(label)|\n",
      "+----------+----------+\n",
      "|    2011.0|    1922.0|\n",
      "+----------+----------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "#Min and max year\n",
    "\n",
    "df.selectExpr('MAX(label)','MIN(label)').show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 59,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Row(label=79.0, features=DenseVector([0.8841, 0.6105, 0.6005, 0.4747, 0.2472, 0.3573, 0.3441, 0.3396, 0.6009, 0.4257, 0.6049, 0.4192]))"
      ]
     },
     "execution_count": 59,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#Shift labels to start from zero\n",
    "\n",
    "parsed_df = df.select(col('label')-1922, 'features')\\\n",
    "              .withColumnRenamed(\"(label - 1922)\",'label')\n",
    "parsed_df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 60,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "DataFrame[label: double, features: vector]"
      ]
     },
     "execution_count": 60,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#Split data into train, test and validation sets and cache in memory\n",
    "\n",
    "train_df, val_df, test_df = parsed_df.randomSplit([0.8,0.1,0.1])\n",
    "\n",
    "train_df.cache() \n",
    "val_df.cache() \n",
    "test_df.cache()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Simple Model\n",
    "\n",
    "A really simplistic model would be to make the same predictions for every song based on the mean year of the training set."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 61,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "baseline = train_df.agg({\"label\":\"mean\"}).map(lambda x: x[0]).collect()[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 62,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Baseline Test RMSE is equal to 21.4445049742\n"
     ]
    }
   ],
   "source": [
    "evaluator = RegressionEvaluator(predictionCol=\"prediction\")\n",
    "\n",
    "baseline_pred_label_df = train_df.select('label').withColumn('prediction',lit(baseline))\n",
    "\n",
    "print \"Baseline Test RMSE is equal to %s\" %(evaluator.evaluate(baseline_pred_label_df)) "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true
   },
   "source": [
    "#Linear Regression\n",
    "## Gradient Descent \"by hand\"\n",
    "\n",
    "Let's try to build a better model by buillding a linear regression model using Gradient Descent"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 63,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "from pyspark.mllib.linalg import DenseVector\n",
    "\n",
    "evaluator = RegressionEvaluator(predictionCol=\"prediction\")\n",
    "\n",
    "\n",
    "def gradient_summand(weights, lp):\n",
    "    \"\"\"Calculates the gradient summand for a given weight and `LabeledPoint`.\"\"\"\n",
    "    summand = DenseVector((DenseVector.dot(lp.features,weights) - lp.label)*lp.features)\n",
    "    return summand\n",
    "\n",
    "def get_labeled_prediction(weights, observation):\n",
    "    \"\"\"Calculates predictions given a tuple of (labeledpoint,features) \n",
    "       and returns a (prediction, label) tuple.\"\"\"\n",
    "    \n",
    "    prediction = float(DenseVector.dot(DenseVector(weights),observation.features))\n",
    "    label = float(observation.label)\n",
    "    \n",
    "    return prediction,label"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 66,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(0.0, 8.0)"
      ]
     },
     "execution_count": 66,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "d = len(train_df.first().features)\n",
    "w = np.zeros(d)\n",
    "train_df.map(lambda x: get_labeled_prediction(w,x)).first()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 67,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "DenseVector([-1.3589, -2.7773, -2.4829, -1.4645, -2.5551, -4.9228, -1.8748, -5.2439, -2.4979, -4.9767, -2.9213, -3.2111])"
      ]
     },
     "execution_count": 67,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "train_df.map(lambda x: gradient_summand(w,x)).first()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 68,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(array([ 25.30056292,  23.57321944,  -3.52286096,   8.73888557,\n",
       "          6.40118176,  -9.10995724,  18.62334359,   2.5340433 ,\n",
       "          9.75836441,   5.17339603,  11.92707673,   1.58719582]),\n",
       " array([  57.91871582,  105.32777714,  111.41327896,   77.46751342,\n",
       "          39.77503786,   22.8138827 ,   20.34489152,   20.17510328,\n",
       "          20.10439782,   20.03989297,   19.97978689,   19.92344627,\n",
       "          19.87037463,   19.82017393,   19.77251927,   19.72714152,\n",
       "          19.68381475,   19.64234721,   19.6025745 ,   19.56435446,\n",
       "          19.52756317,   19.49209187,   19.45784452,   19.42473577,\n",
       "          19.39268943,   19.36163715,   19.3315173 ,   19.30227415,\n",
       "          19.27385708,   19.24621996,   19.21932061,   19.19312037,\n",
       "          19.16758368,   19.14267775,   19.11837229,   19.09463922,\n",
       "          19.07145251,   19.0487879 ,   19.02662281,   19.00493615,\n",
       "          18.98370818,   18.96292043,   18.94255553,   18.92259719,\n",
       "          18.90303007,   18.88383971,   18.86501245,   18.84653541,\n",
       "          18.8283964 ,   18.81058386,   18.79308685,   18.77589497,\n",
       "          18.75899836,   18.74238763,   18.72605385,   18.7099885 ,\n",
       "          18.69418348,   18.67863103,   18.66332377,   18.64825463,\n",
       "          18.63341685,   18.61880396,   18.60440977,   18.59022833,\n",
       "          18.57625395,   18.56248116,   18.54890471,   18.53551957,\n",
       "          18.52232087,   18.50930396,   18.49646435,   18.48379772,\n",
       "          18.4712999 ,   18.45896687,   18.44679476,   18.43477985,\n",
       "          18.42291851,   18.41120728,   18.39964278,   18.38822176,\n",
       "          18.37694109,   18.36579771,   18.35478869,   18.34391117,\n",
       "          18.33316239,   18.32253969,   18.31204046,   18.3016622 ,\n",
       "          18.29140248,   18.28125892,   18.27122924,   18.2613112 ,\n",
       "          18.25150266,   18.24180149,   18.23220568,   18.22271322,\n",
       "          18.2133222 ,   18.20403074,   18.19483702,   18.18573925]))"
      ]
     },
     "execution_count": 68,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "def linreg_gradient_descent(train_data, num_iters):\n",
    "    \"\"\"Calculates the weights and error for a linear regression model trained with gradient descent.\n",
    "\n",
    "    Returns a tuple of (weights, training errors).  Weights will be the\n",
    "            final weights (one weight per feature) for the model, and training errors will contain\n",
    "            an error (RMSE) for each iteration of the algorithm.\n",
    "    \"\"\"\n",
    "    # The length of the training data\n",
    "    n = train_data.count()\n",
    "    # The number of features in the training data\n",
    "    d = len(train_data.first().features)\n",
    "    w = np.zeros(d)\n",
    "    alpha = 1.0\n",
    "    # We will compute and store the training error after each iteration\n",
    "    error_train = np.zeros(num_iters)\n",
    "    for i in range(num_iters):\n",
    "        preds_and_labels_train = train_data.map(lambda x: get_labeled_prediction(w,x))\n",
    "        preds_and_labels_train_df = preds_and_labels_train.toDF([\"prediction\", \"label\"])\n",
    "        error_train[i] = evaluator.evaluate(preds_and_labels_train_df)\n",
    "\n",
    "        # Calculate the `gradient`\n",
    "        gradient = train_data.map(lambda x: gradient_summand(w,x)).sum()\n",
    "\n",
    "        # Update the weights\n",
    "        alpha_i = alpha / (n * np.sqrt(i+1))\n",
    "        w = w - alpha_i * gradient\n",
    "        \n",
    "    return w, error_train\n",
    "    \n",
    "\n",
    "linreg_gradient_descent(train_df, 100)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "###Train the model\n",
    "Now let's train a linear regression model on all of our training data and evaluate its accuracy on the validation set.\n",
    "Note that the test set will not be used here. If we evaluated the model on the test set, we would bias our final results."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 70,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Validation RMSE:\n",
      "\tBaseline = 53.803\n",
      "\tLR0 = 18.750\n"
     ]
    }
   ],
   "source": [
    "num_iters = 50\n",
    "weights_LR0, error_train_LR0 = linreg_gradient_descent(train_df,num_iters)\n",
    "\n",
    "preds_and_labels = (val_df\n",
    "                      .map(lambda x: get_labeled_prediction(weights_LR0,x)))\n",
    "preds_and_labels_df = sqlContext.createDataFrame(preds_and_labels, [\"prediction\", \"label\"])\n",
    "rmse_val_LR0 = calc_RMSE(preds_and_labels_df)\n",
    "\n",
    "print 'Validation RMSE:\\n\\tBaseline = {0:.3f}\\n\\tLR0 = {1:.3f}'.format(baseline,\n",
    "                                                                       rmse_val_LR0)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# MLlib implemenatation\n",
    "\n",
    "Initial regression model performs better than the baseline. Let's try to improve upon it by adding an intercept using regularisation and training for more iterations.\n",
    "\n",
    "This is easily done using Spark ML."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 71,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[22.4880976787,24.8978042167,-66.6704408186,55.1077952952,-11.018847603,-51.108501839,33.8875399236,-21.0174969304,2.22124062781,-2.56259588659,-10.4484262894,-14.1509286227] 65.6848081131\n"
     ]
    }
   ],
   "source": [
    "num_iters = 500  # iterations\n",
    "reg = 1e-1  # regParam\n",
    "alpha = .2  # elasticNetParam\n",
    "use_intercept = True  # intercept\n",
    "\n",
    "\n",
    "lin_reg = LinearRegression(maxIter=num_iters, regParam=reg, \n",
    "                           elasticNetParam=0.1, fitIntercept=True)\n",
    "first_model = lin_reg.fit(train_df)\n",
    "\n",
    "# coeffsLR1 stores the model coefficients; interceptLR1 stores the model intercept\n",
    "coeffs_LR1 = first_model.coefficients\n",
    "intercept_LR1 = first_model.intercept\n",
    "print coeffs_LR1, intercept_LR1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 75,
   "metadata": {
    "collapsed": false,
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "validation = first_model.transform(val_df).select('prediction','label')\n",
    "rmse_val_LR1 = evaluator.evaluate(validation)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 83,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Validation RMSE: \n",
      "\t\tBaseline = 53.8025171201\n",
      "\t\tLR0 = 18.7495636219\n",
      "\t\tLR1 = 15.2884373949\n"
     ]
    }
   ],
   "source": [
    "print 'Validation RMSE: \\n\\t\\tBaseline = %s\\n\\t\\tLR0 = %s\\n\\t\\tLR1 = %s'%(baseline, rmse_val_LR0, rmse_val_LR1)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Grid Search\n",
    "\n",
    "Although the Spark ML model has a smaller mean squared error than the others it's performance may be improved by doing a grid search to find a better regularisation parameter"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 102,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Best Reg Parameter: \n",
      "\t\t1e-10 \n",
      "\n",
      "Validation RMSE:\n",
      "\t\tBaseline = 53.803\n",
      "\t\tLR0 = 18.750\n",
      "\t\tLR1 = 15.288\n",
      "\t\tLRGrid = 15.284\n"
     ]
    }
   ],
   "source": [
    "best_RMSE = rmse_val_LR1\n",
    "best_reg_param = reg\n",
    "best_model = first_model\n",
    "\n",
    "num_iters = 500  # iterations\n",
    "alpha = .2  # elasticNetParam\n",
    "use_intercept = True  # intercept\n",
    "\n",
    "for reg in [1e-10,1e-5,.99]:\n",
    "    lin_reg = LinearRegression(maxIter=num_iters, regParam=reg, elasticNetParam=alpha, fitIntercept=use_intercept)\n",
    "    model = lin_reg.fit(train_df)\n",
    "    val_pred_df = model.transform(val_df)\n",
    "\n",
    "    rmse_val_grid = evaluator.evaluate(val_pred_df)\n",
    "\n",
    "    if rmse_val_grid < best_RMSE:\n",
    "        best_RMSE = rmse_val_grid\n",
    "        best_reg_param = reg\n",
    "        best_model = model\n",
    "\n",
    "rmse_val_LR_grid = best_RMSE\n",
    "\n",
    "\n",
    "print 'Best Reg Parameter: \\n\\t\\t%s \\n'%best_reg_param\n",
    "print ('Validation RMSE:\\n\\t\\tBaseline = {0:.3f}\\n\\t\\tLR0 = {1:.3f}\\n\\t\\tLR1 = {2:.3f}\\n' +\n",
    "       '\\t\\tLRGrid = {3:.3f}').format(baseline, rmse_val_LR0, rmse_val_LR1, rmse_val_LR_grid)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#Interaction terms\n",
    "\n",
    "Add interaction terms to the model."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 103,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "from itertools import product\n",
    "\n",
    "def two_way_interactions(lp):\n",
    "    \"\"\"Creates a new `LabeledPoint` that includes two-way interactions.\n",
    "\n",
    "    Note:\n",
    "        For features [x, y] the two-way interactions would be [x^2, x*y, y*x, y^2] and these\n",
    "        would be appended to the original [x, y] feature list.\n",
    "\n",
    "    Args:\n",
    "        lp (LabeledPoint): The label and features for this observation.\n",
    "\n",
    "    Returns:\n",
    "        LabeledPoint: The new `LabeledPoint` should have the same label as `lp`.  Its features\n",
    "            should include the features from `lp` followed by the two-way interaction features.\n",
    "    \"\"\"\n",
    "    i_j = list(product(range(len(lp.features)),range(len(lp.features))))\n",
    "    \n",
    "    two_way = LabeledPoint(lp.label, np.hstack((lp.features,[lp.features[i] * lp.features[j] for (i,j) in i_j])))\n",
    "    \n",
    "    return two_way"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 104,
   "metadata": {
    "collapsed": false,
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "two_way_train_df = train_df.map(lambda x: LabeledPoint(x[0],x[1]))\\\n",
    "                           .map(lambda x: two_way_interactions(x)).toDF(['features','label'])\n",
    "two_way_val_df = val_df.map(lambda x: LabeledPoint(x[0],x[1]))\\\n",
    "                           .map(lambda x: two_way_interactions(x)).toDF(['features','label'])\n",
    "two_way_test_df = test_df.map(lambda x: LabeledPoint(x[0],x[1]))\\\n",
    "                           .map(lambda x: two_way_interactions(x)).toDF(['features','label'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 105,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Row(features=DenseVector([0.1699, 0.3472, 0.3104, 0.1831, 0.3194, 0.6153, 0.2343, 0.6555, 0.3122, 0.6221, 0.3652, 0.4014, 0.0289, 0.059, 0.0527, 0.0311, 0.0543, 0.1045, 0.0398, 0.1113, 0.053, 0.1057, 0.062, 0.0682, 0.059, 0.1205, 0.1077, 0.0636, 0.1109, 0.2136, 0.0814, 0.2276, 0.1084, 0.216, 0.1268, 0.1393, 0.0527, 0.1077, 0.0963, 0.0568, 0.0991, 0.191, 0.0727, 0.2034, 0.0969, 0.1931, 0.1133, 0.1246, 0.0311, 0.0636, 0.0568, 0.0335, 0.0585, 0.1127, 0.0429, 0.12, 0.0572, 0.1139, 0.0669, 0.0735, 0.0543, 0.1109, 0.0991, 0.0585, 0.102, 0.1965, 0.0748, 0.2094, 0.0997, 0.1987, 0.1166, 0.1282, 0.1045, 0.2136, 0.191, 0.1127, 0.1965, 0.3787, 0.1442, 0.4034, 0.1921, 0.3828, 0.2247, 0.247, 0.0398, 0.0814, 0.0727, 0.0429, 0.0748, 0.1442, 0.0549, 0.1536, 0.0732, 0.1458, 0.0856, 0.0941, 0.1113, 0.2276, 0.2034, 0.12, 0.2094, 0.4034, 0.1536, 0.4297, 0.2047, 0.4078, 0.2394, 0.2631, 0.053, 0.1084, 0.0969, 0.0572, 0.0997, 0.1921, 0.0732, 0.2047, 0.0975, 0.1942, 0.114, 0.1253, 0.1057, 0.216, 0.1931, 0.1139, 0.1987, 0.3828, 0.1458, 0.4078, 0.1942, 0.387, 0.2272, 0.2497, 0.062, 0.1268, 0.1133, 0.0669, 0.1166, 0.2247, 0.0856, 0.2394, 0.114, 0.2272, 0.1333, 0.1466, 0.0682, 0.1393, 0.1246, 0.0735, 0.1282, 0.247, 0.0941, 0.2631, 0.1253, 0.2497, 0.1466, 0.1611]), label=8.0)"
      ]
     },
     "execution_count": 105,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "two_way_train_df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 107,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Validation RMSE:\n",
      "\tBaseline = 53.803\n",
      "\tLR0 = 18.750\n",
      "\tLR1 = 15.288\n",
      "\tLRGrid = 15.284\n",
      "\tLRInteract = 14.276\n"
     ]
    }
   ],
   "source": [
    "num_iters = 500\n",
    "reg = 1e-10\n",
    "alpha = .2\n",
    "use_intercept = True\n",
    "\n",
    "lin_reg = LinearRegression(maxIter=num_iters, regParam=reg, elasticNetParam=alpha, fitIntercept=use_intercept)\n",
    "model_interact = lin_reg.fit(two_way_train_df)\n",
    "preds_and_labels_interact_df = model_interact.transform(two_way_val_df)\n",
    "rmse_val_interact = evaluator.evaluate(preds_and_labels_interact_df)\n",
    "\n",
    "print ('Validation RMSE:\\n\\tBaseline = {0:.3f}\\n\\tLR0 = {1:.3f}\\n\\tLR1 = {2:.3f}\\n\\tLRGrid = ' +\n",
    "       '{3:.3f}\\n\\tLRInteract = {4:.3f}').format(baseline, rmse_val_LR0, rmse_val_LR1,\n",
    "                                                 rmse_val_LR_grid, rmse_val_interact)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Evaluate the performace of the interaction model on the test dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 109,
   "metadata": {
    "collapsed": false,
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Test RMSE:\n",
      "\tBaseline = 53.803\n",
      "\tLRInteract = 15.394\n"
     ]
    }
   ],
   "source": [
    "preds_and_labels_test_df = model_interact.transform(two_way_test_df)\n",
    "rmse_test_interact = evaluator.evaluate(preds_and_labels_test_df)\n",
    "\n",
    "print ('Test RMSE:\\n\\tBaseline = {0:.3f}\\n\\tLRInteract = {1:.3f}'\n",
    "       .format(baseline, rmse_test_interact))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The complete final model can be implemented using Spark ML's pipelines"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "num_iters = 500\n",
    "reg = 1e-10\n",
    "alpha = .2\n",
    "use_intercept = True\n",
    "\n",
    "polynomial_expansion = PolynomialExpansion(degree=2, \n",
    "                                           inputCol=\"features\", \n",
    "                                           outputCol=\"polyFeatures\")\n",
    "linear_regression = LinearRegression(maxIter=num_iters, regParam=reg, elasticNetParam=alpha,\n",
    "                                     fitIntercept=use_intercept, featuresCol='polyFeatures')\n",
    "\n",
    "pipeline = Pipeline(stages=[polynomial_expansion,linear_regression])\n",
    "pipeline_model = pipeline.fit(train_df)\n",
    "\n",
    "predictions_df = pipeline_model.transform(test_df)\n",
    "\n",
    "evaluator = RegressionEvaluator()\n",
    "rmse_test_pipeline = evaluator.evaluate(predictions_df, {evaluator.metricName: \"rmse\"})\n",
    "print('RMSE for test data set using pipelines: {0:.3f}'.format(rmse_test_pipeline))"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 2",
   "language": "python",
   "name": "python2"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.11"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
